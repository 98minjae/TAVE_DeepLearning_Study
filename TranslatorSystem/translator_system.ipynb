{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translator_system.ipynb",
      "provenance": [],
      "mount_file_id": "1TUTwuns3LTILoAmbT6d38AYTo8FRsWcA",
      "authorship_tag": "ABX9TyO+naPvv9XpnsaeafI1RPbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reccos7/TAVE_DeepLearning_Study/blob/main/translator_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7gNVVcGF4qN",
        "outputId": "1eb656da-49ca-4fa2-987e-5a08ce850092"
      },
      "source": [
        "\"\"\"\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.6.0\n",
            "Uninstalling tensorflow-2.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.41.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.8.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.6.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=16d4fec5b670b4bfa0c3d6589e3f5f73e094f127b6ea85919c3b745063c57373\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH_KDH0Bs3fP",
        "outputId": "3d4d73a4-0e15-426f-f8af-f4ebffd77be1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-FHij1myioT",
        "outputId": "7a59aa4a-3e27-445c-8f2b-193327e3230f"
      },
      "source": [
        "!pip install nltk\n",
        "from nltk.corpus import comtrans\n",
        "import nltk\n",
        "nltk.download('comtrans',download_dir='/content/drive/MyDrive/Colab_Notebooks/TAVE_DL_Study/translator_system')\n",
        "nltk.data.path.append('/content/drive/MyDrive/Colab_Notebooks/TAVE_DL_Study/translator_system')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "[nltk_data] Downloading package comtrans to /content/drive/MyDrive/Col\n",
            "[nltk_data]     ab_Notebooks/TAVE_DL_Study/translator_system...\n",
            "[nltk_data]   Unzipping corpora/comtrans.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHliz-p4yHYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b39aa98-5db0-499f-b1f7-ae6bc8bf1043"
      },
      "source": [
        "print(comtrans.aligned_sents('alignment-de-en.txt')[0]) #말뭉치\n",
        "print(comtrans.aligned_sents()[0].words) #첫번째언어\n",
        "print(comtrans.aligned_sents()[0].mots) #두번째언어\n",
        "print(comtrans.aligned_sents()[0].alignment) #단어매칭"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<AlignedSent: 'Wiederaufnahme der S...' -> 'Resumption of the se...'>\n",
            "['Wiederaufnahme', 'der', 'Sitzungsperiode']\n",
            "['Resumption', 'of', 'the', 'session']\n",
            "0-0 1-1 1-2 2-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2AxFBiJkJWj"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import random\n",
        "import numpy as np\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVOwTF2q1Csn"
      },
      "source": [
        "def retrieve_corpora(translated_sentences_l1_l2='alignment-de-en.txt'):\n",
        "    #말뭉치 검색\n",
        "    print(\"Retrieving corpora: {}\".format(translated_sentences_l1_l2))\n",
        "    als = comtrans.aligned_sents(translated_sentences_l1_l2)\n",
        "    sentences_l1 = [sent.words for sent in als]\n",
        "    sentences_l2 = [sent.mots for sent in als]\n",
        "    return sentences_l1, sentences_l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3731jRV1C_6",
        "outputId": "bc5b0852-88a8-43f8-f121-8602eb77bda5"
      },
      "source": [
        "sen_l1, sen_l2 = retrieve_corpora()\n",
        "print(\"# A sentence in the two languages DE & EN\")\n",
        "print(\"DE:\", sen_l1[0])\n",
        "print(\"EN:\", sen_l2[0])\n",
        "print(\"# Corpora length (i.e. number of sentences)\")\n",
        "print(len(sen_l1))\n",
        "assert len(sen_l1) == len(sen_l2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving corpora: alignment-de-en.txt\n",
            "# A sentence in the two languages DE & EN\n",
            "DE: ['Wiederaufnahme', 'der', 'Sitzungsperiode']\n",
            "EN: ['Resumption', 'of', 'the', 'session']\n",
            "# Corpora length (i.e. number of sentences)\n",
            "33334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-g2uA-81ElS"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "    #구두점을 토큰화하고 토큰을 소문자로 변환\n",
        "    regex_splitter = re.compile(\"([!?.,:;$\\\"')( ])\")\n",
        "    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n",
        "    return [w for words in clean_words for w in words if words if w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17Kqp0_U1Kka",
        "outputId": "cfb2dd5d-eaac-4076-a4fa-513732b4c485"
      },
      "source": [
        "clean_sen_l1 = [clean_sentence(s) for s in sen_l1]\n",
        "clean_sen_l2 = [clean_sentence(s) for s in sen_l2]\n",
        "print(\"# Same sentence as before, but chunked and cleaned\")\n",
        "print(\"DE:\", clean_sen_l1[0])\n",
        "print(\"EN:\", clean_sen_l2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Same sentence as before, but chunked and cleaned\n",
            "DE: ['wiederaufnahme', 'der', 'sitzungsperiode']\n",
            "EN: ['resumption', 'of', 'the', 'session']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjAewK0C1MKg"
      },
      "source": [
        "def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n",
        "    # 토큰 개수가 N개보다 크면 문장이 삭제\n",
        "    filtered_sentences_l1 = []\n",
        "    filtered_sentences_l2 = []\n",
        "    for i in range(len(sentences_l1)):\n",
        "        if min_len <= len(sentences_l1[i]) <= max_len and \\\n",
        "                                min_len <= len(sentences_l2[i]) <= max_len:\n",
        "            filtered_sentences_l1.append(sentences_l1[i])\n",
        "            filtered_sentences_l2.append(sentences_l2[i])\n",
        "    return filtered_sentences_l1, filtered_sentences_l2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulNycERK1Nuw",
        "outputId": "5651b0d9-cca5-4783-9232-de7cf84f5b3d"
      },
      "source": [
        "filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2)\n",
        "print(\"# Filtered Corpora length (i.e. number of sentences)\")\n",
        "print(len(filt_clean_sen_l1))\n",
        "assert len(filt_clean_sen_l1) == len(filt_clean_sen_l2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Filtered Corpora length (i.e. number of sentences)\n",
            "14788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fz1d3ulEW45"
      },
      "source": [
        "_PAD = \"_PAD\"\n",
        "_GO = \"_GO\"\n",
        "_EOS = \"_EOS\"\n",
        "_UNK = \"_UNK\"\n",
        "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
        "\n",
        "PAD_ID = 0\n",
        "GO_ID = 1\n",
        "EOS_ID = 2\n",
        "UNK_ID = 3\n",
        "OP_DICT_IDS = [PAD_ID, GO_ID, EOS_ID, UNK_ID]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKOCP4tD1PEe"
      },
      "source": [
        "def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n",
        "    #사전 생성\n",
        "    count_words = Counter()\n",
        "    dict_words = {}\n",
        "    opt_dict_size = len(OP_DICT_IDS)\n",
        "    for sen in sentences:\n",
        "        for word in sen:\n",
        "            count_words[word] += 1\n",
        "\n",
        "    dict_words[_PAD] = PAD_ID\n",
        "    dict_words[_GO] = GO_ID\n",
        "    dict_words[_EOS] = EOS_ID\n",
        "    dict_words[_UNK] = UNK_ID\n",
        "\n",
        "    for idx, item in enumerate(count_words.most_common(dict_size)):\n",
        "        dict_words[item[0]] = idx + opt_dict_size\n",
        "\n",
        "    if not os.path.exists(os.path.dirname(storage_path)):\n",
        "        os.makedirs(os.path.dirname(storage_path))\n",
        "    else:\n",
        "        pickle.dump(dict_words, open(storage_path, \"wb\"))\n",
        "    return dict_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoC2y02w1bAW"
      },
      "source": [
        "def sentences_to_indexes(sentences, indexed_dictionary):\n",
        "    # 토큰을 ID로 대체\n",
        "    indexed_sentences = []\n",
        "    not_found_counter = 0\n",
        "    for sent in sentences:\n",
        "        idx_sent = []\n",
        "        for word in sent:\n",
        "            try:\n",
        "                idx_sent.append(indexed_dictionary[word])\n",
        "            except KeyError:\n",
        "                idx_sent.append(UNK_ID)\n",
        "                not_found_counter += 1\n",
        "        indexed_sentences.append(idx_sent)\n",
        "\n",
        "    print('[sentences_to_indexes] Did not find {} words'.format(not_found_counter))\n",
        "    return indexed_sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zVrP_Nr1kqq",
        "outputId": "462aba88-7e01-44d8-f943-922e502a0120"
      },
      "source": [
        "dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=\"/tmp/l1_dict.p\")\n",
        "dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=\"/tmp/l2_dict.p\")\n",
        "\n",
        "idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
        "idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
        "\n",
        "print(\"# Same sentences as before, with their dictionary ID\")\n",
        "print(\"DE:\", list(zip(filt_clean_sen_l1[0], idx_sentences_l1[0])))\n",
        "print(\"EN:\", list(zip(filt_clean_sen_l2[0], idx_sentences_l2[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sentences_to_indexes] Did not find 1097 words\n",
            "[sentences_to_indexes] Did not find 0 words\n",
            "# Same sentences as before, with their dictionary ID\n",
            "DE: [('wiederaufnahme', 1616), ('der', 7), ('sitzungsperiode', 618)]\n",
            "EN: [('resumption', 1779), ('of', 8), ('the', 5), ('session', 549)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-EWmnfU1miJ"
      },
      "source": [
        "def extract_max_length(corpora):\n",
        "    #문장의 최대길이\n",
        "    return max([len(sentence) for sentence in corpora])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtbbLrGjFAVh",
        "outputId": "44013df6-723e-4eb4-9d07-1d819995cefa"
      },
      "source": [
        "max_length_l1 = extract_max_length(idx_sentences_l1)\n",
        "max_length_l2 = extract_max_length(idx_sentences_l2)\n",
        "print(\"# Max sentence sizes:\")\n",
        "print(\"DE:\", max_length_l1)\n",
        "print(\"EN:\", max_length_l2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Max sentence sizes:\n",
            "DE: 20\n",
            "EN: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReylI5Y2FBxs"
      },
      "source": [
        "def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n",
        "    \"\"\"\n",
        "    입력 시퀀스를 전부 다 해서 20개의 기호로 채움\n",
        "    출력 시퀀스에 20개의 기호를 채움\n",
        "    번역문의 시작과 끝 위치를 지정하기 위해 출력 시뭔스의 시작 부분에 _GO를 추가하고 끝부분에 _EOS를 추가\n",
        "    \"\"\"\n",
        "    assert len(sentences_l1) == len(sentences_l2)\n",
        "    data_set = []\n",
        "    for i in range(len(sentences_l1)):\n",
        "        padding_l1 = len_l1 - len(sentences_l1[i])\n",
        "        pad_sentence_l1 = ([PAD_ID]*padding_l1) + sentences_l1[i]\n",
        "\n",
        "        padding_l2 = len_l2 - len(sentences_l2[i])\n",
        "        pad_sentence_l2 = [GO_ID] + sentences_l2[i] + [EOS_ID] + ([PAD_ID] * padding_l2)\n",
        "        data_set.append([pad_sentence_l1, pad_sentence_l2])\n",
        "\n",
        "    return data_set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4TsiGS1FECn",
        "outputId": "a10e1a4a-b22e-4680-bed6-232c084e19d0"
      },
      "source": [
        "data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n",
        "print(\"# Prepared minibatch with paddings and extra stuff\")\n",
        "print(\"DE:\", data_set[0][0])\n",
        "print(\"EN:\", data_set[0][1])\n",
        "print(\"# The sentence pass from X to Y tokens\")\n",
        "print(\"DE:\", len(idx_sentences_l1[0]), \"->\", len(data_set[0][0]))\n",
        "print(\"EN:\", len(idx_sentences_l2[0]), \"->\", len(data_set[0][1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Prepared minibatch with paddings and extra stuff\n",
            "DE: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1616, 7, 618]\n",
            "EN: [1, 1779, 8, 5, 549, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "# The sentence pass from X to Y tokens\n",
            "DE: 3 -> 20\n",
            "EN: 4 -> 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcphSZ8lFGdT"
      },
      "source": [
        "class Seq2SeqModel(object):\n",
        "  \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n",
        "  This class implements a multi-layer recurrent neural network as encoder,\n",
        "  and an attention-based decoder. This is the same as the model described in\n",
        "  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n",
        "  or into the seq2seq library for complete model implementation.\n",
        "  This class also allows to use GRU cells in addition to LSTM cells, and\n",
        "  sampled softmax to handle large output vocabulary size. A single-layer\n",
        "  version of this model, but with bi-directional encoder, was presented in\n",
        "    http://arxiv.org/abs/1409.0473\n",
        "  and sampled softmax is described in Section 3 of the following paper.\n",
        "    http://arxiv.org/abs/1412.2007\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               source_vocab_size,\n",
        "               target_vocab_size,\n",
        "               buckets,\n",
        "               size,\n",
        "               num_layers,\n",
        "               max_gradient_norm,\n",
        "               batch_size,\n",
        "               learning_rate,\n",
        "               learning_rate_decay_factor,\n",
        "               use_lstm=False,\n",
        "               num_samples=512,\n",
        "               forward_only=False,\n",
        "               dtype=tf.float32):\n",
        "    \"\"\"Create the model.\n",
        "    Args:\n",
        "      source_vocab_size: size of the source vocabulary.\n",
        "      target_vocab_size: size of the target vocabulary.\n",
        "      buckets: a list of pairs (I, O), where I specifies maximum input length\n",
        "        that will be processed in that bucket, and O specifies maximum output\n",
        "        length. Training instances that have inputs longer than I or outputs\n",
        "        longer than O will be pushed to the next bucket and padded accordingly.\n",
        "        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n",
        "      size: number of units in each layer of the model.\n",
        "      num_layers: number of layers in the model.\n",
        "      max_gradient_norm: gradients will be clipped to maximally this norm.\n",
        "      batch_size: the size of the batches used during training;\n",
        "        the model construction is independent of batch_size, so it can be\n",
        "        changed after initialization if this is convenient, e.g., for decoding.\n",
        "      learning_rate: learning rate to start with.\n",
        "      learning_rate_decay_factor: decay learning rate by this much when needed.\n",
        "      use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
        "      num_samples: number of samples for sampled softmax.\n",
        "      forward_only: if set, we do not construct the backward pass in the model.\n",
        "      dtype: the data type to use to store internal variables.\n",
        "    \"\"\"\n",
        "    self.source_vocab_size = source_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.buckets = buckets\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = tf.Variable(\n",
        "        float(learning_rate), trainable=False, dtype=dtype)\n",
        "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
        "        self.learning_rate * learning_rate_decay_factor)\n",
        "    self.global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # If we use sampled softmax, we need an output projection.\n",
        "    output_projection = None\n",
        "    softmax_loss_function = None\n",
        "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
        "    if num_samples > 0 and num_samples < self.target_vocab_size:\n",
        "      w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n",
        "      w = tf.transpose(w_t)\n",
        "      b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n",
        "      output_projection = (w, b)\n",
        "\n",
        "      def sampled_loss(labels, logits):\n",
        "        labels = tf.reshape(labels, [-1, 1])\n",
        "        # We need to compute the sampled_softmax_loss using 32bit floats to\n",
        "        # avoid numerical instabilities.\n",
        "        local_w_t = tf.cast(w_t, tf.float32)\n",
        "        local_b = tf.cast(b, tf.float32)\n",
        "        local_inputs = tf.cast(logits, tf.float32)\n",
        "        return tf.cast(\n",
        "            tf.nn.sampled_softmax_loss(\n",
        "                weights=local_w_t,\n",
        "                biases=local_b,\n",
        "                labels=labels,\n",
        "                inputs=local_inputs,\n",
        "                num_sampled=num_samples,\n",
        "                num_classes=self.target_vocab_size),\n",
        "            dtype)\n",
        "      softmax_loss_function = sampled_loss\n",
        "\n",
        "    # Create the internal multi-layer cell for our RNN.\n",
        "    def single_cell():\n",
        "      return tf.contrib.rnn.GRUCell(size)\n",
        "    if use_lstm:\n",
        "      def single_cell():\n",
        "        return tf.contrib.rnn.BasicLSTMCell(size)\n",
        "    cell = single_cell()\n",
        "    if num_layers > 1:\n",
        "      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n",
        "\n",
        "    # The seq2seq function: we use embedding for the input and attention.\n",
        "    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
        "      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
        "          encoder_inputs,\n",
        "          decoder_inputs,\n",
        "          cell,\n",
        "          num_encoder_symbols=source_vocab_size,\n",
        "          num_decoder_symbols=target_vocab_size,\n",
        "          embedding_size=size,\n",
        "          output_projection=output_projection,\n",
        "          feed_previous=do_decode,\n",
        "          dtype=dtype)\n",
        "\n",
        "    # Feeds for inputs.\n",
        "    self.encoder_inputs = []\n",
        "    self.decoder_inputs = []\n",
        "    self.target_weights = []\n",
        "    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
        "      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"encoder{0}\".format(i)))\n",
        "    for i in xrange(buckets[-1][1] + 1):\n",
        "      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"decoder{0}\".format(i)))\n",
        "      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
        "                                                name=\"weight{0}\".format(i)))\n",
        "\n",
        "    # Our targets are decoder inputs shifted by one.\n",
        "    targets = [self.decoder_inputs[i + 1]\n",
        "               for i in xrange(len(self.decoder_inputs) - 1)]\n",
        "\n",
        "    # Training outputs and losses.\n",
        "    if forward_only:\n",
        "      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
        "          self.encoder_inputs, self.decoder_inputs, targets,\n",
        "          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
        "          softmax_loss_function=softmax_loss_function)\n",
        "      # If we use output projection, we need to project outputs for decoding.\n",
        "      if output_projection is not None:\n",
        "        for b in xrange(len(buckets)):\n",
        "          self.outputs[b] = [\n",
        "              tf.matmul(output, output_projection[0]) + output_projection[1]\n",
        "              for output in self.outputs[b]\n",
        "          ]\n",
        "    else:\n",
        "      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
        "          self.encoder_inputs, self.decoder_inputs, targets,\n",
        "          self.target_weights, buckets,\n",
        "          lambda x, y: seq2seq_f(x, y, False),\n",
        "          softmax_loss_function=softmax_loss_function)\n",
        "\n",
        "    # Gradients and SGD update operation for training the model.\n",
        "    params = tf.trainable_variables()\n",
        "    if not forward_only:\n",
        "      self.gradient_norms = []\n",
        "      self.updates = []\n",
        "      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
        "      for b in xrange(len(buckets)):\n",
        "        gradients = tf.gradients(self.losses[b], params)\n",
        "        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
        "                                                         max_gradient_norm)\n",
        "        self.gradient_norms.append(norm)\n",
        "        self.updates.append(opt.apply_gradients(\n",
        "            zip(clipped_gradients, params), global_step=self.global_step))\n",
        "\n",
        "    self.saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
        "           bucket_id, forward_only):\n",
        "    \"\"\"Run a step of the model feeding the given inputs.\n",
        "    Args:\n",
        "      session: tensorflow session to use.\n",
        "      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
        "      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
        "      target_weights: list of numpy float vectors to feed as target weights.\n",
        "      bucket_id: which bucket of the model to use.\n",
        "      forward_only: whether to do the backward step or only forward.\n",
        "    Returns:\n",
        "      A triple consisting of gradient norm (or None if we did not do backward),\n",
        "      average perplexity, and the outputs.\n",
        "    Raises:\n",
        "      ValueError: if length of encoder_inputs, decoder_inputs, or\n",
        "        target_weights disagrees with bucket size for the specified bucket_id.\n",
        "    \"\"\"\n",
        "    # Check if the sizes match.\n",
        "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
        "    if len(encoder_inputs) != encoder_size:\n",
        "      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
        "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
        "    if len(decoder_inputs) != decoder_size:\n",
        "      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
        "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
        "    if len(target_weights) != decoder_size:\n",
        "      raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
        "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
        "\n",
        "    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
        "    input_feed = {}\n",
        "    for l in xrange(encoder_size):\n",
        "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
        "    for l in xrange(decoder_size):\n",
        "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
        "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
        "\n",
        "    # Since our targets are decoder inputs shifted by one, we need one more.\n",
        "    last_target = self.decoder_inputs[decoder_size].name\n",
        "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
        "\n",
        "    # Output feed: depends on whether we do a backward step or not.\n",
        "    if not forward_only:\n",
        "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
        "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
        "                     self.losses[bucket_id]]  # Loss for this batch.\n",
        "    else:\n",
        "      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
        "      for l in xrange(decoder_size):  # Output logits.\n",
        "        output_feed.append(self.outputs[bucket_id][l])\n",
        "\n",
        "    outputs = session.run(output_feed, input_feed)\n",
        "    if not forward_only:\n",
        "      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
        "    else:\n",
        "      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
        "\n",
        "  def get_batch(self, data, bucket_id):\n",
        "    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n",
        "    To feed data in step(..) it must be a list of batch-major vectors, while\n",
        "    data here contains single length-major cases. So the main logic of this\n",
        "    function is to re-index data cases to be in the proper format for feeding.\n",
        "    Args:\n",
        "      data: a tuple of size len(self.buckets) in which each element contains\n",
        "        lists of pairs of input and output data that we use to create a batch.\n",
        "      bucket_id: integer, which bucket to get the batch for.\n",
        "    Returns:\n",
        "      The triple (encoder_inputs, decoder_inputs, target_weights) for\n",
        "      the constructed batch that has the proper format to call step(...) later.\n",
        "    \"\"\"\n",
        "    encoder_size, decoder_size = self.buckets[bucket_id]\n",
        "    encoder_inputs, decoder_inputs = [], []\n",
        "\n",
        "    # Get a random batch of encoder and decoder inputs from data,\n",
        "    # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
        "    for _ in xrange(self.batch_size):\n",
        "      encoder_input, decoder_input = random.choice(data[bucket_id])\n",
        "\n",
        "      # Encoder inputs are padded and then reversed.\n",
        "      encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
        "      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
        "\n",
        "      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
        "      decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
        "      decoder_inputs.append([GO_ID] + decoder_input +\n",
        "                            [PAD_ID] * decoder_pad_size)\n",
        "\n",
        "    # Now we create batch-major vectors from the data selected above.\n",
        "    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
        "\n",
        "    # Batch encoder inputs are just re-indexed encoder_inputs.\n",
        "    for length_idx in xrange(encoder_size):\n",
        "      batch_encoder_inputs.append(\n",
        "          np.array([encoder_inputs[batch_idx][length_idx]\n",
        "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
        "\n",
        "    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
        "    for length_idx in xrange(decoder_size):\n",
        "      batch_decoder_inputs.append(\n",
        "          np.array([decoder_inputs[batch_idx][length_idx]\n",
        "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
        "\n",
        "      # Create target_weights to be 0 for targets that are padding.\n",
        "      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
        "      for batch_idx in xrange(self.batch_size):\n",
        "        # We set weight to 0 if the corresponding target is a PAD symbol.\n",
        "        # The corresponding target is decoder_input shifted by 1 forward.\n",
        "        if length_idx < decoder_size - 1:\n",
        "          target = decoder_inputs[batch_idx][length_idx + 1]\n",
        "        if length_idx == decoder_size - 1 or target == PAD_ID:\n",
        "          batch_weight[batch_idx] = 0.0\n",
        "      batch_weights.append(batch_weight)\n",
        "    return batch_encoder_inputs, batch_decoder_inputs, batch_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbDkgdYHG8qQ"
      },
      "source": [
        "path_l1_dict = \"/tmp/l1_dict.p\"\n",
        "path_l2_dict = \"/tmp/l2_dict.p\"\n",
        "model_dir = \"/tmp/translate\"\n",
        "model_checkpoints = model_dir + \"/translate.ckpt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmPC2c62HTH0"
      },
      "source": [
        "def build_dataset(use_stored_dictionary=False):\n",
        "    \"\"\"\n",
        "    인수가 False라면 처음부터 사전을 구성해서 저장하고, 그렇지 않으면 해당 경로에 있는 사전을 사용\n",
        "    정리된 문장, 데이터셋, 문장의 최대 길이, 사전의 길이를 변환\n",
        "    \"\"\"\n",
        "    sen_l1, sen_l2 = retrieve_corpora()\n",
        "    clean_sen_l1 = [clean_sentence(s) for s in sen_l1]\n",
        "    clean_sen_l2 = [clean_sentence(s) for s in sen_l2]\n",
        "    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2)\n",
        "\n",
        "    if not use_stored_dictionary:\n",
        "        dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=15000, storage_path=path_l1_dict)\n",
        "        dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=10000, storage_path=path_l2_dict)\n",
        "    else:\n",
        "        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n",
        "        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
        "\n",
        "    dict_l1_length = len(dict_l1)\n",
        "    dict_l2_length = len(dict_l2)\n",
        "\n",
        "    idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n",
        "    idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n",
        "\n",
        "    max_length_l1 = extract_max_length(idx_sentences_l1)\n",
        "    max_length_l2 = extract_max_length(idx_sentences_l2)\n",
        "    data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n",
        "\n",
        "    return (filt_clean_sen_l1, filt_clean_sen_l2), \\\n",
        "           data_set, \\\n",
        "           (max_length_l1, max_length_l2), \\\n",
        "           (dict_l1_length, dict_l2_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFtu1W3FHV7_"
      },
      "source": [
        "def cleanup_checkpoints(model_dir, model_checkpoints):\n",
        "    \"\"\"\n",
        "    훈련 루틴을 실행할 때마다 모델 디렉터리를 정리해서 쓰레기 정보를 제공하지 않게 한다\n",
        "    \"\"\"\n",
        "    for f in glob.glob(model_checkpoints + \"*\"):\n",
        "        os.remove(f)\n",
        "    try:\n",
        "        os.mkdir(model_dir)\n",
        "    except FileExistsError:\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rc6hvhoHYuw"
      },
      "source": [
        "def get_seq2seq_model(session, forward_only, dict_lengths, max_sentence_lengths, model_dir):\n",
        "    \"\"\"\n",
        "    재사용 가능한 형태로 모델 생성\n",
        "    모델 생성자를 호출해서 (원어 어휘 크기, 번역어 어휘 크기, 버킷, 장단기 메모리 내부 유닛 크기, 쌓인 LSTM 계층 개수,\n",
        "    경사도 최대 노름, 미니 배치 크기, 학습 속도, 학습 속도 감소 계수, 모델 방향, 데이터 형식) 전달\n",
        "    if/else문: 모델이 이미 존재한다면 체크포인트에서 모델을 검색\n",
        "    \"\"\"\n",
        "    model = Seq2SeqModel(\n",
        "        source_vocab_size=dict_lengths[0],\n",
        "        target_vocab_size=dict_lengths[1],\n",
        "        buckets=[max_sentence_lengths],\n",
        "        size=256,\n",
        "        num_layers=2,\n",
        "        max_gradient_norm=5.0,\n",
        "        batch_size=64,\n",
        "        learning_rate=0.5,\n",
        "        learning_rate_decay_factor=0.99,\n",
        "        forward_only=forward_only,\n",
        "        dtype=tf.float16)\n",
        "    ckpt = tf.train.get_checkpoint_state(model_dir)\n",
        "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
        "        print(\"Reading model parameters from {}\".format(ckpt.model_checkpoint_path))\n",
        "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        print(\"Created model with fresh parameters.\")\n",
        "        session.run(tf.global_variables_initializer())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4df32C_YHa5m"
      },
      "source": [
        "def train():\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        model = get_seq2seq_model(sess, False, dict_lengths, max_sentence_lengths, model_dir)\n",
        "\n",
        "        # This is the training loop.\n",
        "        step_time, loss = 0.0, 0.0\n",
        "        current_step = 0\n",
        "        bucket = 0\n",
        "        steps_per_checkpoint = 100\n",
        "        max_steps = 20000\n",
        "\n",
        "        while current_step < max_steps:\n",
        "\n",
        "            start_time = time.time()\n",
        "            encoder_inputs, decoder_inputs, target_weights = model.get_batch([data_set], bucket)\n",
        "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket, False)\n",
        "            step_time += (time.time() - start_time) / steps_per_checkpoint\n",
        "            loss += step_loss / steps_per_checkpoint\n",
        "            current_step += 1\n",
        "\n",
        "            if current_step % steps_per_checkpoint == 0:\n",
        "                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
        "                print (\"global step {} learning rate {} step-time {} perplexity {}\".format(\n",
        "                       model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n",
        "\n",
        "                sess.run(model.learning_rate_decay_op)\n",
        "\n",
        "                model.saver.save(sess, model_checkpoints, global_step=model.global_step)\n",
        "                step_time, loss = 0.0, 0.0\n",
        "\n",
        "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([data_set], bucket)\n",
        "                _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket, True)\n",
        "                eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
        "                print(\"  eval: perplexity {}\".format(eval_ppx))\n",
        "                sys.stdout.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "RHA4h-vKHdXn",
        "outputId": "c9642464-175b-46d5-9c5f-5e902a60f5e8"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    _, data_set, max_sentence_lengths, dict_lengths = build_dataset(False)\n",
        "    cleanup_checkpoints(model_dir, model_checkpoints)\n",
        "    tf.reset_default_graph()\n",
        "    train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving corpora: alignment-de-en.txt\n",
            "[sentences_to_indexes] Did not find 1097 words\n",
            "[sentences_to_indexes] Did not find 0 words\n",
            "Created model with fresh parameters.\n",
            "global step 100 learning rate 0.5 step-time 16.500172679424285 perplexity 589.9046640574632\n",
            "  eval: perplexity 180.4245166495895\n",
            "global step 200 learning rate 0.4951171875 step-time 16.39452571630479 perplexity 138.15806627941603\n",
            "  eval: perplexity 115.58428452718766\n",
            "global step 300 learning rate 0.490234375 step-time 16.31250190258025 perplexity 108.95527751430792\n",
            "  eval: perplexity 89.66618776422116\n",
            "global step 400 learning rate 0.4853515625 step-time 16.099205341339108 perplexity 93.50798886022528\n",
            "  eval: perplexity 85.2265069597304\n",
            "global step 500 learning rate 0.480712890625 step-time 16.012383956909176 perplexity 84.66573187982435\n",
            "  eval: perplexity 72.89809449073893\n",
            "global step 600 learning rate 0.47607421875 step-time 16.261856241226198 perplexity 75.9175920986028\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "  eval: perplexity 74.62682298320091\n",
            "global step 700 learning rate 0.471435546875 step-time 16.258564054965973 perplexity 72.27995730247987\n",
            "  eval: perplexity 76.39654709045266\n",
            "global step 800 learning rate 0.466796875 step-time 16.127364013195034 perplexity 67.70606628193029\n",
            "  eval: perplexity 59.964336869699544\n",
            "global step 900 learning rate 0.462158203125 step-time 16.21065758943557 perplexity 62.224087741290944\n",
            "  eval: perplexity 57.442316329639176\n",
            "global step 1000 learning rate 0.457763671875 step-time 16.1171453332901 perplexity 58.2205439480726\n",
            "  eval: perplexity 57.66713919909174\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-966b72b1b05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcleanup_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-295f1c1c6240>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-413c74ca30e9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvgMbd6HIUOz"
      },
      "source": [
        "model_dir = \"/tmp/translate\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd2At5qGIXSx"
      },
      "source": [
        "def decode():\n",
        "  with tf.Session() as sess:\n",
        "    model = get_seq2seq_model(sess, True, dict_lengths, max_sentence_lengths, model_dir)\n",
        "    model.batch_size = 1\n",
        "    bucket = 0\n",
        "\n",
        "    for idx in range(len(data_set))[:5]:\n",
        "        print(\"-------------------\")\n",
        "        print(\"Source sentence: \", sentences[0][idx])\n",
        "        print(\"Source tokens: \", data_set[idx][0])\n",
        "        print(\"Ideal tokens out: \", data_set[idx][1])\n",
        "        print(\"Ideal sentence out: \", sentences[1][idx])\n",
        "\n",
        "        encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          {bucket: [(data_set[idx][0], [])]}, bucket)\n",
        "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket, True)\n",
        "        outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "        if EOS_ID in outputs:\n",
        "            outputs = outputs[1:outputs.index(EOS_ID)]\n",
        "\n",
        "        print(\"Model output: \",  \" \".join([tf.compat.as_str(inv_dict_l2[output]) for output in outputs]))\n",
        "        sys.stdout.flush()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxf3Ce9CIXyk",
        "outputId": "529ada7c-18fa-4f25-ef8e-c4a46e616e62"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n",
        "    inv_dict_l2 = {v: k for k, v in dict_l2.items()}\n",
        "\n",
        "    build_dataset(True)\n",
        "    sentences, data_set, max_sentence_lengths, dict_lengths = build_dataset(False)\n",
        "    try:\n",
        "        print(\"Reading from\", model_dir)\n",
        "        print(\"Dictionary lengths\", dict_lengths)\n",
        "        print(\"Bucket size\", max_sentence_lengths)\n",
        "    except NameError:\n",
        "        print(\"One or more variables not in scope. Translation not possible\")\n",
        "        exit(-1)\n",
        "    tf.reset_default_graph()\n",
        "    decode()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving corpora: alignment-de-en.txt\n",
            "[sentences_to_indexes] Did not find 1097 words\n",
            "[sentences_to_indexes] Did not find 0 words\n",
            "Retrieving corpora: alignment-de-en.txt\n",
            "[sentences_to_indexes] Did not find 1097 words\n",
            "[sentences_to_indexes] Did not find 0 words\n",
            "Reading from /tmp/translate\n",
            "Dictionary lengths (15004, 9852)\n",
            "Bucket size (20, 20)\n",
            "WARNING:tensorflow:From <ipython-input-33-63f2d7895abf>:21: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Reading model parameters from /tmp/translate/translate.ckpt-1000\n",
            "INFO:tensorflow:Restoring parameters from /tmp/translate/translate.ckpt-1000\n",
            "-------------------\n",
            "Source sentence:  ['wiederaufnahme', 'der', 'sitzungsperiode']\n",
            "Source tokens:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1616, 7, 618]\n",
            "Ideal tokens out:  [1, 1779, 8, 5, 549, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Ideal sentence out:  ['resumption', 'of', 'the', 'session']\n",
            "Model output:  agenda of the\n",
            "-------------------\n",
            "Source sentence:  ['ich', 'bitte', 'sie', ',', 'sich', 'zu', 'einer', 'schweigeminute', 'zu', 'erheben', '.']\n",
            "Source tokens:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 266, 22, 5, 29, 14, 78, 3931, 14, 2414, 4]\n",
            "Ideal tokens out:  [1, 651, 932, 6, 159, 6, 19, 11, 1440, 35, 51, 2639, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Ideal sentence out:  ['please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.']\n",
            "Model output:  i would to to to to to to . . .\n",
            "-------------------\n",
            "Source sentence:  ['(', 'das', 'parlament', 'erhebt', 'sich', 'zu', 'einer', 'schweigeminute', '.', ')']\n",
            "Source tokens:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 52, 11, 58, 3267, 29, 14, 78, 3931, 4, 51]\n",
            "Ideal tokens out:  [1, 54, 5, 267, 3541, 14, 2095, 12, 1440, 35, 51, 2639, 53, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Ideal sentence out:  ['(', 'the', 'house', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')']\n",
            "Model output:  ( parliament parliament the the the the\n",
            "-------------------\n",
            "Source sentence:  ['frau', 'präsidentin', ',', 'zur', 'geschäftsordnung', '.']\n",
            "Source tokens:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 151, 5, 49, 488, 4]\n",
            "Ideal tokens out:  [1, 212, 44, 6, 22, 12, 91, 8, 218, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Ideal sentence out:  ['madam', 'president', ',', 'on', 'a', 'point', 'of', 'order', '.']\n",
            "Model output:  mr president , , , . . .\n",
            "-------------------\n",
            "Source sentence:  ['wenn', 'das', 'haus', 'damit', 'einverstanden', 'ist', ',', 'werde', 'ich', 'dem', 'vorschlag', 'von', 'herrn', 'evans', 'folgen', '.']\n",
            "Source tokens:  [0, 0, 0, 0, 85, 11, 603, 113, 831, 9, 5, 243, 13, 39, 141, 18, 116, 1939, 417, 4]\n",
            "Ideal tokens out:  [1, 87, 5, 267, 2096, 6, 16, 213, 47, 29, 27, 1941, 25, 1441, 4, 2, 0, 0, 0, 0, 0, 0]\n",
            "Ideal sentence out:  ['if', 'the', 'house', 'agrees', ',', 'i', 'shall', 'do', 'as', 'mr', 'evans', 'has', 'suggested', '.']\n",
            "Model output:  _GO i is i i i i i i i i the of of . . . . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TqKdX9FNSmH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
